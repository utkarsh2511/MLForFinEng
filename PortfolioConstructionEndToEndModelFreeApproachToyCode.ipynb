{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEglHtrQ-8vR",
        "outputId": "fa7c2ee3-b213-4100-98f3-abbf4d47846b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a8b337793f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# setting the seed for torch\n",
        "torch.manual_seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the neural network with the given structure\n",
        "class forcasting_model(nn.Module):\n",
        "\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(2, 1, bias=True, device=None, dtype=None)\n",
        "\n",
        "        #initializing the weights and biases\n",
        "        weight = torch.tensor([[0.5, 0.5]])\n",
        "        bias = torch.tensor([0.5])\n",
        "        with torch.no_grad():\n",
        "          self.linear.weight.copy_(weight)\n",
        "          self.linear.bias.copy_(bias)\n",
        "\n",
        "    def forward(self, inp: torch.tensor):\n",
        "        print(\"printing input: \",inp)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        out = F.leaky_relu(inp)\n",
        "        print(\"printing output: of leaky relu \",out)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        out = self.linear(out)\n",
        "        print(\"printing output: of linear layer\",out)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        out = F.softmax(out, dim=0)\n",
        "        print(\"printing output: of softmax layer\",out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "P7tZyZJE_Gvd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the class\n",
        "model = forcasting_model()\n",
        "\n",
        "# weights and biases before training\n",
        "for layer in model.children():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        print(layer.state_dict()['weight'])\n",
        "        print(layer.state_dict()['bias'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm-8SeJf_I33",
        "outputId": "410ceb1d-ad18-4abf-c18c-9772e7ad289b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000, 0.5000]])\n",
            "tensor([0.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the loss function\n",
        "# sharpeRatioLossFunc = lambda out,x,x_star: -((out.T @ x_star)/((out.T @ torch.cov(x))@ out))\n",
        "portfolioReturnLossFunction = lambda out,x,x_star: -((out.T @ x_star))\n",
        "\n",
        "# specifying the optimization algorithm\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n"
      ],
      "metadata": {
        "id": "v-Rqw28Z_RfF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toy input data\n",
        "\n",
        "# Daily returns of two stocks for yesterday and day before yesterday\n",
        "x = torch.tensor([[2.0, 1.0],\n",
        "                  [4.0, -2.0]])\n",
        "\n",
        "# Daily return today\n",
        "x_star = torch.tensor([[4.0],\n",
        "                       [6.0]])"
      ],
      "metadata": {
        "id": "JMDp1uRV_7Ah"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "\n",
        "# training\n",
        "model.train()\n",
        "out = model(x)\n",
        "\n",
        "# calculating loss\n",
        "loss = portfolioReturnLossFunction(out, x, x_star)\n",
        "\n",
        "# backpropagation\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKtMJp17A32m",
        "outputId": "7ef3c4b6-8fd0-4f68-f7b6-e67011c8f263"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "printing input:  tensor([[ 2.,  1.],\n",
            "        [ 4., -2.]])\n",
            "\n",
            "\n",
            "printing output: of leaky relu  tensor([[ 2.0000,  1.0000],\n",
            "        [ 4.0000, -0.0200]])\n",
            "\n",
            "\n",
            "printing output: of linear layer tensor([[2.0000],\n",
            "        [2.4900]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "printing output: of softmax layer tensor([[0.3799],\n",
            "        [0.6201]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-K-D8QI2bnnz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weights and biases after training\n",
        "for layer in model.children():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        print(layer.state_dict()['weight'])\n",
        "        print(layer.state_dict()['bias'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVuD9sVZBFM8",
        "outputId": "78e1580d-adac-43d5-c480-67cde3f5064c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5094, 0.4952]])\n",
            "tensor([0.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "u39IAlo8Bkpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily returns of two stocks for today and yesterday\n",
        "x = torch.tensor([[4.0, 2.0],\n",
        "                  [6.0, 4.0]])"
      ],
      "metadata": {
        "id": "hio-3C7aBWfz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJNyCLSNByQz",
        "outputId": "dbc9e0fe-0c1c-45f4-a8ec-052e15e0c0fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "printing input:  tensor([[4., 2.],\n",
            "        [6., 4.]])\n",
            "\n",
            "\n",
            "printing output: of leaky relu  tensor([[4., 2.],\n",
            "        [6., 4.]])\n",
            "\n",
            "\n",
            "printing output: of linear layer tensor([[3.5281],\n",
            "        [5.5373]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "printing output: of softmax layer tensor([[0.1182],\n",
            "        [0.8818]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OaUlFLEFB19c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}